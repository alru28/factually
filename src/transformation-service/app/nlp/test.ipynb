{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from collections import Counter\n",
    "\n",
    "STORAGE_SERVICE_URL = \"http://localhost:8001\"\n",
    "\n",
    "class NLPProcessor:\n",
    "    \"\"\"\n",
    "    A processor for performing various NLP tasks such as summarization, sentiment analysis,\n",
    "    and zero-shot classification on articles retrieved from a storage service.\n",
    "    \n",
    "    This class handles long texts by chunking them to fit the model's maximum context length,\n",
    "    processes each chunk individually, and then aggregates the results.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the NLPProcessor with pipelines and tokenizers for each task.\n",
    "        Uses task-specific models for summarization, sentiment analysis, and classification.\n",
    "        \"\"\"\n",
    "        self.summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "        self.sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "        self.classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "        self.summarizer_tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "        self.sentiment_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "        self.classify_tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-mnli\")\n",
    "    \n",
    "    def retrieve_article_content(self, article_id: str) -> str:\n",
    "        \"\"\"\n",
    "        Retrieves an article from the storage service via its API and concatenates its title and paragraphs.\n",
    "        \n",
    "        Args:\n",
    "            article_id (str): The unique identifier for the article to retrieve.\n",
    "        \n",
    "        Returns:\n",
    "            str: A string containing the article's title and paragraphs.\n",
    "        \n",
    "        Raises:\n",
    "            Exception: If the article retrieval fails (non-200 response).\n",
    "        \"\"\"\n",
    "        url = f\"{STORAGE_SERVICE_URL}/articles/{article_id}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Failed to retrieve article {article_id}: {response.text}\")\n",
    "        article = response.json()\n",
    "        title = article.get(\"Title\", \"\")\n",
    "        paragraphs = article.get(\"Paragraphs\", [])\n",
    "        content = title + \"\\n\" + \"\\n\".join(paragraphs)\n",
    "        return content\n",
    "    \n",
    "    def summarize(self, article_id: str) -> str:\n",
    "        \"\"\"\n",
    "        Summarizes the content of an article.\n",
    "        \n",
    "        The method retrieves the article content, splits it into manageable chunks based on the\n",
    "        summarization model's context limit, generates partial summaries for each chunk, and then\n",
    "        combines and optionally refines these partial summaries into a final summary.\n",
    "        \n",
    "        Args:\n",
    "            article_id (str): The unique identifier for the article to summarize.\n",
    "            \n",
    "        Returns:\n",
    "            str: A summary of the article.\n",
    "        \n",
    "        Raises:\n",
    "            Exception: If the article has no content.\n",
    "        \"\"\"\n",
    "        content = self.retrieve_article_content(article_id)\n",
    "\n",
    "        if not content:\n",
    "            raise Exception(f\"Article {article_id} has no content to summarize.\")\n",
    "\n",
    "        chunks = self.chunk_text(text=content, task=\"summarization\")\n",
    "\n",
    "        partial_summaries = [\n",
    "            self.summarizer(chunk, max_length=200, min_length=100, do_sample=False)[0][\"summary_text\"]\n",
    "            for chunk in chunks\n",
    "        ]\n",
    "\n",
    "        combined_summary = \" \".join(partial_summaries)\n",
    "        \n",
    "        if len(self.summarizer_tokenizer.tokenize(combined_summary)) > 1024:\n",
    "            final_summary = self.summarizer(combined_summary, max_length=400, min_length=200, do_sample=False)[0][\"summary_text\"]\n",
    "        else:\n",
    "            final_summary = combined_summary\n",
    "\n",
    "        return final_summary\n",
    "    \n",
    "    def chunk_text(self, text: str, task: str) -> list:\n",
    "        \"\"\"\n",
    "        Splits the input text into chunks that do not exceed the model's maximum token limit for a given task.\n",
    "        \n",
    "        The function uses different tokenizers and token limits based on the task:\n",
    "          - \"summarization\": Uses summarizer_tokenizer with a max of 1024 tokens.\n",
    "          - \"sentiment\": Uses sentiment_tokenizer with a max of 512 tokens.\n",
    "          - \"classification\": Uses classify_tokenizer with a max of 1024 tokens.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The text to be chunked.\n",
    "            task (str): The task for which the text is being processed. Must be one of \"summarization\",\n",
    "                        \"sentiment\", or \"classification\".\n",
    "            \n",
    "        Returns:\n",
    "            list: A list of text chunks, each within the specified token limit.\n",
    "        \"\"\"\n",
    "        sentences = text.split(\". \")\n",
    "        chunks, current_chunk = [], \"\"\n",
    "\n",
    "        if task == \"summarization\":\n",
    "            tokenizer = self.summarizer_tokenizer\n",
    "            max_tokens=1024\n",
    "        elif task == \"sentiment\":\n",
    "            tokenizer = self.sentiment_tokenizer\n",
    "            max_tokens=512\n",
    "        elif task == \"classification\":\n",
    "            tokenizer = self.classify_tokenizer\n",
    "            max_tokens=1024\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported task specified. Use 'summarization', 'sentiment', or 'classification'.\")\n",
    "\n",
    "        for sentence in sentences:\n",
    "            potential_chunk = current_chunk + sentence + \". \"\n",
    "            token_len = len(tokenizer.tokenize(potential_chunk))\n",
    "\n",
    "            if token_len > max_tokens:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence + \". \"\n",
    "            else:\n",
    "                current_chunk = potential_chunk\n",
    "\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "        return chunks\n",
    "    \n",
    "    def analyze_sentiment(self, article_id: str) -> dict:\n",
    "        \"\"\"\n",
    "        Performs sentiment analysis on an article.\n",
    "        \n",
    "        The method retrieves the article content, splits it into chunks to respect the sentiment model's\n",
    "        context length, performs sentiment analysis on each chunk, and aggregates the results using a\n",
    "        majority vote for the label and an average of the confidence scores.\n",
    "        \n",
    "        Args:\n",
    "            article_id (str): The unique identifier for the article to analyze.\n",
    "            \n",
    "        Returns:\n",
    "            dict: A dictionary with keys \"label\" and \"score\", representing the overall sentiment.\n",
    "        \n",
    "        Raises:\n",
    "            Exception: If the article has no content.\n",
    "        \"\"\"\n",
    "        content = self.retrieve_article_content(article_id)\n",
    "        if not content:\n",
    "            raise Exception(f\"Article {article_id} has no content for sentiment analysis.\")\n",
    "        \n",
    "        chunks = self.chunk_text(text=content, task=\"sentiment\")\n",
    "        sentiments = [self.sentiment_analyzer(chunk)[0] for chunk in chunks]\n",
    "        \n",
    "        labels = [result[\"label\"] for result in sentiments]\n",
    "        avg_score = sum(result[\"score\"] for result in sentiments) / len(sentiments)\n",
    "        \n",
    "        majority_label = Counter(labels).most_common(1)[0][0]\n",
    "        return {\"label\": majority_label, \"score\": avg_score}\n",
    "    \n",
    "    def classify(self, article_id: str, candidate_labels: list = None) -> dict:\n",
    "        \"\"\"\n",
    "        Classifies an article into one of the candidate labels using zero-shot classification.\n",
    "        \n",
    "        The method retrieves the article content, splits it into chunks to respect the classifier's\n",
    "        context length, performs classification on each chunk, aggregates the scores for each candidate\n",
    "        label across all chunks, and then normalizes the scores to determine the final classification.\n",
    "        \n",
    "        Args:\n",
    "            article_id (str): The unique identifier for the article to classify.\n",
    "            candidate_labels (list, optional): A list of candidate labels for classification.\n",
    "                Defaults to [\"economics\", \"sports\", \"entertainment\", \"politics\", \"technology\", \"culture\", \"\"].\n",
    "                \n",
    "        Returns:\n",
    "            dict: A dictionary containing the final label under the key \"label\" and the normalized scores\n",
    "                  for each candidate label under the key \"scores\".\n",
    "            \n",
    "        Raises:\n",
    "            Exception: If the article has no content.\n",
    "        \"\"\"\n",
    "        if candidate_labels is None:\n",
    "            candidate_labels = [\"economics\", \"sports\", \"entertainment\", \"politics\", \"technology\", \"culture\", \"\"]\n",
    "    \n",
    "        content = self.retrieve_article_content(article_id)\n",
    "        if not content:\n",
    "            raise Exception(f\"Article {article_id} has no content to classify.\")\n",
    "        \n",
    "        chunks = self.chunk_text(content, task=\"classification\")\n",
    "        \n",
    "        aggregated_scores = {label: 0 for label in candidate_labels}\n",
    "        for chunk in chunks:\n",
    "            result = self.classifier(chunk, candidate_labels)\n",
    "            for label, score in zip(result[\"labels\"], result[\"scores\"]):\n",
    "                aggregated_scores[label] += score\n",
    "        \n",
    "        total = sum(aggregated_scores.values())\n",
    "        normalized_scores = {label: score / total for label, score in aggregated_scores.items()}\n",
    "        final_label = max(normalized_scores, key=normalized_scores.get)\n",
    "        return {\"label\": final_label, \"scores\": normalized_scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (574 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: 6,000 employees were let go at the USDA in February as part of a government-wide purge. The USDA cuts are being felt especially in coastal states home to major shipping ports. Experts warn that the losses could cause food to go rotten while waiting in ports and could lead to even higher grocery prices, in addition to increasing the chances of potentially devastating invasive species getting into the country. Two federal judges and an independent agency that assesses government personnel have already ordered that fired USDA employees be reinstated. Fired USDA workers are still waiting to hear whether they will be reinstated. The Trump administration has signaled it will fight court decisions to reinstate employees. Customs and Border Protection deploys the dogs trained by Copeland and other staffers at the National Dog Detection Training Center. The two agencies run the Agricultural Quarantine Inspection program, but it’s funded by the USDA and not taxpayer dollars. The USDA said that it was pausing the terminations for 45 days and would “develop a phased plan for return-to-duty”\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1075 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: {'label': 'NEGATIVE', 'score': 0.9987107714017233}\n",
      "Classification: {'label': 'economics', 'scores': {'economics': 0.29504014075041657, 'sports': 0.110580816005517, 'entertainment': 0.11551535631807783, 'politics': 0.08210918610448754, 'technology': 0.050974502472549824, 'culture': 0.20001639872887128, '': 0.14576359962007998}}\n"
     ]
    }
   ],
   "source": [
    "nlp_processor = NLPProcessor()\n",
    "test_article_id = \"ec7f043c-7701-4843-aad3-023a283df5d9\"\n",
    "try:\n",
    "    summary = nlp_processor.summarize(test_article_id)\n",
    "    print(\"Summary:\", summary)\n",
    "    \n",
    "    sentiment = nlp_processor.analyze_sentiment(test_article_id)\n",
    "    print(\"Sentiment:\", sentiment)\n",
    "    \n",
    "    classification = nlp_processor.classify(test_article_id)\n",
    "    print(\"Classification:\", classification)\n",
    "except Exception as e:\n",
    "    print(\"Error:\", str(e))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
